{"cells":[{"cell_type":"code","source":["# Colab: Mount into drive\n","from google.colab import drive\n","drive.mount(\"/content/drive\")\n","#place this tutorial.ipynb in your google drive under below directories (of course you need to create these folders first!):\n","#/SideProjects/LLM/SgLang/\n","%cd '/content/drive/MyDrive/SideProjects/LLM/SgLang/'"],"metadata":{"id":"Cq8Q913e68eG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#import all necassary packages:\n","! pip install --upgrade pip\n","! pip install \"sglang[all]\"\n","! pip install flashinfer -i https://flashinfer.ai/whl/cu121/torch2.4/\n","! pip install triton"],"metadata":{"id":"Cwwmtyo2HGhj","collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#log into hugging face: this is used to connect with the selected language model\n","!huggingface-cli login"],"metadata":{"id":"YBogbv23s1sO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#used to running the server (you can choose any, but I recommend start with the parameter with 8b!) in the background, so that we can run the next cell in the colab\n","#we need to wait one or two minutes before running the next cell!\n","import subprocess\n","subprocess.Popen(['python', '-m', 'sglang.launch_server', '--model-path', 'meta-llama/Meta-Llama-3-8B-Instruct', '--port', '30000'])\n"],"metadata":{"id":"ZZT6iPxQtDCj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#check if the server is running or not:\n","!ps -aux | grep sglang.launch_server"],"metadata":{"id":"GxVuqpIDtW0Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#this is an example to show the main component of sglang.\n","\n","#all neccasarry packages to be used in sglang, you can add some others as well!\n","from sglang import function, system, user, assistant, gen, set_default_backend, RuntimeEndpoint\n","\n","#this is just the framwork!\n","#we have the system about the background set up\n","# we have user, and it is used to give some questions\n","#we have assistant, this is used to generate some answers, and we can assign any variable as answer here!\n","@function\n","def multi_turn_question(s, question_1, question_2):\n","    s += system(\"You are a helpful assistant.\")\n","    s += user(question_1)\n","    s += assistant(gen(\"answer_1\", max_tokens=256))\n","    s += user(question_2)\n","    s += assistant(gen(\"answer_2\", max_tokens=256))\n","\n","#this is used to call the server and run it locally!\n","set_default_backend(RuntimeEndpoint(\"http://localhost:30000\"))\n","\n","#this is just call the function we had!\n","q1 = \"What is the capital of the United States?\"\n","q2 = \"List two local attractions.\"\n","state = multi_turn_question.run(\n","    question_1=q1,\n","    question_2=q2,\n",")\n","\n","#this is some printed results, and we can figure out what's going on for this variable?\n","print(\"\\n========== all in raw format ==========\\n\")\n","print(state)\n","\n","print(\"\\n========== useful content ==========\\n\")\n","for m in state.messages():\n","    print(m)\n","\n","print(\"\\n========== in certain structure ==========\\n\")\n","for m in state.messages():\n","    print(m[\"role\"], \":\", m[\"content\"])\n","\n","print(\"\\n========== question 1 with answer ==========\\n\")\n","print(q1 + '\\n')\n","print(state[\"answer_1\"])\n","\n","print(\"\\n========== question 2 with answer ==========\\n\")\n","print(q2 + '\\n')\n","print(state[\"answer_2\"])\n"],"metadata":{"id":"_mRNNyiGJtI2"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"private_outputs":true,"provenance":[{"file_id":"11ccMZ3iwXo054SeYSge6_k2IHM5lXJTt","timestamp":1685999055998}],"machine_shape":"hm","gpuType":"A100"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.9"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}